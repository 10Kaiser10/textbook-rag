import os
from langchain_groq import ChatGroq
from data.summaries.sound_summary import SOUND_SUMMARY
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.tools import tool
from rag.pipeline import ReActRAG
from langchain_core.runnables import Runnable
from operator import itemgetter
from rag.pinecone_query import PineconeImageRetriever
from pydantic import BaseModel, Field
from typing import Literal

class RagRouteQuery(BaseModel):
    """Route a user query to either the vectorstore or the LLM"""
    datasource: Literal["use RAG vectorstore", "use LLM directly"] = Field(
        ...,
        description="Given a user question choose to route it to RAG dataase (1) or a direct response from LLM (0).",
    )

def get_summary(inp):
        return SOUND_SUMMARY

class Agent:
    def __init__(
            self, groq_api_key, langchain_api_key, 
            rag_retriever,
            image_retriever,
            model='llama-3.1-8b-instant',
            langchain_project="textbook-rag", enable_tracing='true', langchain_endpoint="https://api.smith.langchain.com"
            ):
        
        self.init_langsmith(langchain_api_key=langchain_api_key, langchain_project=langchain_project, enable_tracing=enable_tracing, langchain_endpoint=langchain_endpoint)

        self.generator_llm = ChatGroq(model=model, temperature=1, max_retries=2, api_key=groq_api_key)
        self.router_llm = ChatGroq(model=model, temperature=1, max_retries=2, api_key=groq_api_key)
        self.rag = ReActRAG(rag_retriever, self.generator_llm, self.router_llm, langchain_api_key)

        self.image_retriever = image_retriever
        
    def init_langsmith(self, langchain_api_key, langchain_project="textbook-rag", enable_tracing='true', langchain_endpoint="https://api.smith.langchain.com"):
        os.environ["LANGCHAIN_TRACING_V2"] = enable_tracing
        os.environ["LANGCHAIN_ENDPOINT"] = langchain_endpoint
        os.environ["LANGCHAIN_API_KEY"] = langchain_api_key
        os.environ["LANGCHAIN_PROJECT"] = langchain_project

    def image_caption_response(self, answer, image_desc, init_query):
        prompt = """
            You are an expert agent who answers a query from a Human, by first generating a text answer and finding a related image, and then combining this information into a final response.
            You are given:
              - A query by a Human.
            You have already produced:
              - A text answer to the query generated by a LLM (written below)
              - An image related to the answer whose short description is written below
            You were given the query and is trying to response to it.
            Generate a final response to the user query using your text answer and the image description.
            Make sure to add a line or two in the answer which connects the final answer to the image.
            Only output the final response and nothing else. Dont say things like "here's the final response" etc

            Query: {0}
            Text Answer: {1}
            Image description: {2}
        """.format(init_query, answer, image_desc)
        return self.generator_llm.invoke(prompt).content

    def direct_llm_response(self, text):
        return self.generator_llm.invoke(text).content

    def rag_router_call(self, text):
        structured_llm_router = self.router_llm.with_structured_output(RagRouteQuery)
        prompt = """
            You are an expert at deciding whether a user query should be answered using a RAG vectorstore or directly by an LLM.

            The RAG vectorstore contains a summary of a textbook, which includes:
            - A short summary of the textbook.
            - A detailed structure of the textbook, covering all chapters, topics, and subtopics.
            - A list of keywords, names, and concepts found in the textbook.

            Your task is to determine the best approach to answering the query:
            1. **Use the LLM directly** if:
            - The query is a simple greeting or casual phrase (e.g., "Hello", "How are you?", "What's up?").
            - The query is unrelated to the textbook topics, keywords, or concepts listed.
            2. **Use the RAG vectorstore** if the query is clearly related to the textbook content and requires specific information from the textbook.

            Make your decision strictly based on whether the query aligns with the textbook topics, subtopics, or keywords. For simple queries or unrelated topics, respond using the LLM directly.

            List of topics and keywords from the textbook:
            {1}

            Query:
            {0}
        """
        return structured_llm_router.invoke(prompt.format(text, SOUND_SUMMARY)).datasource
    
    def react_rag_response(self, text):
        return self.rag.get_response(text)
    
    def query(self, text_input):
        rag_router_response = self.rag_router_call(text_input)

        print(rag_router_response)

        #return using LLM if selected
        if rag_router_response == "use LLM directly":
            return self.direct_llm_response(text_input), (None, None)
        
        #get rag response, get image path, update response based on image
        rag_response = self.react_rag_response(text_input)
        img_path, img_desc, img_scr = self.image_retriever.get_relevant_image(rag_response)[0]
        final_response = self.image_caption_response(rag_response, img_desc, text_input)

        return final_response, (img_path, img_scr)
